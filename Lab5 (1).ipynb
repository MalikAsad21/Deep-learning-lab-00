{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries and Define Activation Functions"
      ],
      "metadata": {
        "id": "E1chF5bQRFMR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Define the sigmoid function and its derivative\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1 - sigmoid(x))\n",
        "\n",
        "# Define the ReLU function and its derivative\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return np.where(x > 0, 1, 0)\n"
      ],
      "metadata": {
        "id": "rx1a0_N9RFW4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Initialize Weights, Biases, Input, and True Output"
      ],
      "metadata": {
        "id": "HvMHAUueRFgD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "w1, b1 = 0.5, 0\n",
        "w2, b2 = 0.5, 0\n",
        "w3, b3 = 0.5, 0\n",
        "\n",
        "# Input and true output\n",
        "x = 1\n",
        "y = 1\n"
      ],
      "metadata": {
        "id": "KUKYxNNvRFqO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigmoid Network – Forward and Backward Pass"
      ],
      "metadata": {
        "id": "W1IAF5TzRFzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Sigmoid Network:\")\n",
        "\n",
        "# Forward pass (Sigmoid)\n",
        "z1_sigmoid = w1 * x + b1\n",
        "a1_sigmoid = sigmoid(z1_sigmoid)\n",
        "\n",
        "z2_sigmoid = w2 * a1_sigmoid + b2\n",
        "a2_sigmoid = sigmoid(z2_sigmoid)\n",
        "\n",
        "z3_sigmoid = w3 * a2_sigmoid + b3\n",
        "y_pred_sigmoid = sigmoid(z3_sigmoid)\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "loss_sigmoid = 0.5 * (y - y_pred_sigmoid) ** 2\n",
        "\n",
        "# Backward pass (Gradient calculation for Sigmoid)\n",
        "# Output layer\n",
        "dL_dy_pred_sigmoid = -(y - y_pred_sigmoid)\n",
        "dy_pred_dz3_sigmoid = sigmoid_derivative(z3_sigmoid)\n",
        "dL_dw3_sigmoid = dL_dy_pred_sigmoid * dy_pred_dz3_sigmoid * a2_sigmoid\n",
        "\n",
        "# Layer 2\n",
        "dL_da2_sigmoid = dL_dy_pred_sigmoid * dy_pred_dz3_sigmoid * w3\n",
        "da2_dz2_sigmoid = sigmoid_derivative(z2_sigmoid)\n",
        "dL_dw2_sigmoid = dL_da2_sigmoid * da2_dz2_sigmoid * a1_sigmoid\n",
        "\n",
        "# Layer 1\n",
        "dL_da1_sigmoid = dL_da2_sigmoid * da2_dz2_sigmoid * w2\n",
        "da1_dz1_sigmoid = sigmoid_derivative(z1_sigmoid)\n",
        "dL_dw1_sigmoid = dL_da1_sigmoid * da1_dz1_sigmoid * x\n",
        "\n",
        "# Print results for Sigmoid\n",
        "print(\"\\nForward Pass (Sigmoid):\")\n",
        "print(f\"Layer 1 Output (a1): {a1_sigmoid:.4f}\")\n",
        "print(f\"Layer 2 Output (a2): {a2_sigmoid:.4f}\")\n",
        "print(f\"Predicted Output (y_pred): {y_pred_sigmoid:.4f}\")\n",
        "print(f\"Loss: {loss_sigmoid:.4f}\")\n",
        "\n",
        "print(\"\\nBackward Pass (Gradients for Sigmoid):\")\n",
        "print(f\"Gradient w.r.t. w3: {dL_dw3_sigmoid:.4f}\")\n",
        "print(f\"Gradient w.r.t. w2: {dL_dw2_sigmoid:.4f}\")\n",
        "print(f\"Gradient w.r.t. w1: {dL_dw1_sigmoid:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AxNh6fjhRF9t",
        "outputId": "cd01f3b4-364f-43b9-bbeb-1b4ae6bc19bb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sigmoid Network:\n",
            "\n",
            "Forward Pass (Sigmoid):\n",
            "Layer 1 Output (a1): 0.6225\n",
            "Layer 2 Output (a2): 0.5772\n",
            "Predicted Output (y_pred): 0.5717\n",
            "Loss: 0.0917\n",
            "\n",
            "Backward Pass (Gradients for Sigmoid):\n",
            "Gradient w.r.t. w3: -0.0605\n",
            "Gradient w.r.t. w2: -0.0080\n",
            "Gradient w.r.t. w1: -0.0015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU Network – Forward and Backward Pass"
      ],
      "metadata": {
        "id": "_8VY2ZhaRGHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nReLU Network:\")\n",
        "\n",
        "# Forward pass (ReLU)\n",
        "z1_relu = w1 * x + b1\n",
        "a1_relu = relu(z1_relu)\n",
        "\n",
        "z2_relu = w2 * a1_relu + b2\n",
        "a2_relu = relu(z2_relu)\n",
        "\n",
        "z3_relu = w3 * a2_relu + b3\n",
        "y_pred_relu = relu(z3_relu)\n",
        "\n",
        "# Loss function (Mean Squared Error)\n",
        "loss_relu = 0.5 * (y - y_pred_relu) ** 2\n",
        "\n",
        "# Backward pass (Gradient calculation for ReLU)\n",
        "# Output layer\n",
        "dL_dy_pred_relu = -(y - y_pred_relu)\n",
        "dy_pred_dz3_relu = relu_derivative(z3_relu)\n",
        "dL_dw3_relu = dL_dy_pred_relu * dy_pred_dz3_relu * a2_relu\n",
        "\n",
        "# Layer 2\n",
        "dL_da2_relu = dL_dy_pred_relu * dy_pred_dz3_relu * w3\n",
        "da2_dz2_relu = relu_derivative(z2_relu)\n",
        "dL_dw2_relu = dL_da2_relu * da2_dz2_relu * a1_relu\n",
        "\n",
        "# Layer 1\n",
        "dL_da1_relu = dL_da2_relu * da2_dz2_relu * w2\n",
        "da1_dz1_relu = relu_derivative(z1_relu)\n",
        "dL_dw1_relu = dL_da1_relu * da1_dz1_relu * x\n",
        "\n",
        "# Print results for ReLU\n",
        "print(\"\\nForward Pass (ReLU):\")\n",
        "print(f\"Layer 1 Output (a1): {a1_relu:.4f}\")\n",
        "print(f\"Layer 2 Output (a2): {a2_relu:.4f}\")\n",
        "print(f\"Predicted Output (y_pred): {y_pred_relu:.4f}\")\n",
        "print(f\"Loss: {loss_relu:.4f}\")\n",
        "\n",
        "print(\"\\nBackward Pass (Gradients for ReLU):\")\n",
        "print(f\"Gradient w.r.t. w3: {dL_dw3_relu:.4f}\")\n",
        "print(f\"Gradient w.r.t. w2: {dL_dw2_relu:.4f}\")\n",
        "print(f\"Gradient w.r.t. w1: {dL_dw1_relu:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmSy8-OSRGSH",
        "outputId": "ec51496e-d80b-40ba-929f-cf7df8901d0f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ReLU Network:\n",
            "\n",
            "Forward Pass (ReLU):\n",
            "Layer 1 Output (a1): 0.5000\n",
            "Layer 2 Output (a2): 0.2500\n",
            "Predicted Output (y_pred): 0.1250\n",
            "Loss: 0.3828\n",
            "\n",
            "Backward Pass (Gradients for ReLU):\n",
            "Gradient w.r.t. w3: -0.2188\n",
            "Gradient w.r.t. w2: -0.2188\n",
            "Gradient w.r.t. w1: -0.2188\n"
          ]
        }
      ]
    }
  ]
}